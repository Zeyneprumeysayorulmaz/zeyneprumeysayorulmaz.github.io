---
title: Kubernetes NotlarÄ±m I- Kubeadm ile K8S Cluster Kurulumu
---

Merhabalar, bu yazÄ±mda kubernetes platformundan ve bileÅŸenlerinden bahsedeceÄŸim, beraberinde kubectl aracÄ±nÄ± kuracak ve kubeadm ile bir kubernetes cluster'Ä± oluÅŸturacaÄŸÄ±m. Bir seri olarak yazmayÄ± planlÄ±yorum. Serinin ilk yazÄ±sÄ± iÃ§in keyifli okumalar.. ğŸ˜Š

Kubernetes, CNCF (Cloud Native Computing Foundation) tarafÄ±ndan yÃ¶netilen (baÅŸlangÄ±Ã§ta Google' Ä±n tasarladÄ±ÄŸÄ±), aÃ§Ä±k kaynak bir container orkestrasyon aracÄ±dÄ±r. AslÄ±nda bakarsanÄ±z iÅŸlevi dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼nde yetersiz bir tanÄ±m oluyor, iÃ§ine girdikÃ§e bu tanÄ±ma eklemeler yapÄ±yor olacaksÄ±nÄ±z.

***Peki container orkestrasyon nedir?***
KÄ±saca, Ã§alÄ±ÅŸan Ã§ok sayÄ±da container bulunduÄŸu durumda onlarÄ± kolayca yÃ¶netmek ve idare etmek iÃ§in bir yol diyebiliriz.

Kubernetes [resmi sitesinde](http://kubernetes.io/docs/concepts/overview/) nasÄ±l tanÄ±mlanÄ±yor bakalÄ±m.


K8S, hem beyan temelli yapÄ±landÄ±rmayÄ± hem de otomasyonu kolaylaÅŸtÄ±ran container iÅŸ yÃ¼klerini ve hizmetlerini yÃ¶netmek iÃ§in oluÅŸturulmuÅŸ, taÅŸÄ±nabilir ve geniÅŸletilebilir aÃ§Ä±k kaynaklÄ± bir platformdur.

SektÃ¶rde standart diyebiliriz, evet alternatifler var fakat orkestrasyona yaklaÅŸÄ±m ÅŸekli ve mimarisi, bir kuruluÅŸa baÄŸlÄ± olmadan vakÄ±f tarafÄ±ndan destekleniyor olmasÄ± gibi sebepler kubernetes'i standard haline getirmiÅŸ diyebiliriz.

# Kubernetes ModÃ¼ler YapÄ±sÄ±
Platform tamamen modÃ¼ler bir ÅŸekilde tasarlanmÄ±ÅŸ ve bu ÅŸekilde de Ã§alÄ±ÅŸÄ±yor. K8s adÄ±nda tek bir uygulama yok, k8s platformunu oluÅŸturan bir Ã§ok servis mevcut ve bu servislerin bir arada dÃ¼zgÃ¼n kurgulanmasÄ± sonucu k8s platformu oluÅŸuyor.

Mimariyi ve modÃ¼ler yapÄ±yÄ± anlamak adÄ±na resmi sitesinde yer alan resim Ã¼zerinden komponentlere ve detayÄ±na bakalÄ±m.

![]({{ 'assets/img/components-of-kubernetes.svg' | relative_url }})


Control planede master node dediÄŸimiz yÃ¶netim nodelarÄ± yer alÄ±yor. **kube-apiserver, etcd, kube-scheduler** ve **kube-controller-manager** komponentleri master node dediÄŸimiz sistemler Ã¼zerinde Ã§alÄ±ÅŸÄ±yor. Master nodelar Ã¼zerinde iÅŸ yÃ¼kÃ¼ Ã§alÄ±ÅŸtÄ±rmayÄ±z, burasÄ± yÃ¶netim altyapÄ±mÄ±zdÄ±r.

DiÄŸer alanda ise worker nodelar Ã§alÄ±ÅŸÄ±yor ve her worker node Ã¼zerinde **kubelet, kube-proxy** ve **container runtime** bileÅŸenleri yer alÄ±yor. Ä°ÅŸ yÃ¼kleri (podlar) bu nodelar Ã¼zerinde Ã§alÄ±ÅŸÄ±r.
## Control Plane BileÅŸenleri
### kube-apiserver
Control plane'in en Ã¶nemli bileÅŸeni, k8s'in beyni diyebiliriz. DiÄŸer tÃ¼m komponentlerin, node bileÅŸenlerinin ve dÄ±ÅŸ dÃ¼nyadan k8s platformu ile iletiÅŸim kuran tÃ¼m servislerin direkt olarak iletiÅŸim kurabildiÄŸi tek komponenttir. K8S'de kaynak oluÅŸturma isteklerinin api doÄŸrulamasÄ±ndan sorumludur. KullanÄ±cÄ±lar kubectl istemcisi ile veya rest api Ã§aÄŸrÄ±larÄ± ile kube-apiserver ile iletiÅŸim kurabilirler.

### etcd
![]({{ 'assets/img/etcd.png' | relative_url }})

TÃ¼m cluster'Ä±n verisi, metadata bilgileri ve K8S' de oluÅŸturulan tÃ¼m objelerin bilgilerinin tutulduÄŸu anahtar-deÄŸer (key-value) veri deposudur. KÄ±sacasÄ± k8s'in Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli olan veriyi Ã¼zerinde tutar.
- Cluster'Ä±n mevcut durumu ile ilgili veriyi Ã¼zerinde barÄ±ndÄ±rÄ±r.
- kube-apiserver hariÃ§ hiÃ§bir bileÅŸen direkt olarak etcd ile iletiÅŸime geÃ§emez. Ä°letiÅŸim gerektiÄŸinde kube-apiserver aracÄ±lÄ±ÄŸÄ±yla bunu yaparlar.

### kube-scheduler
Yeni oluÅŸturulan ya da bir node atamasÄ± yapÄ±lmamÄ±ÅŸ podlarÄ± izler ve Ã¼zerinde Ã§alÄ±ÅŸacaklarÄ± bir node seÃ§er. Pod'un Ã¼zerinde Ã§alÄ±ÅŸmasÄ± iÃ§in en uygun node'a karar verir.

### kube-controller-manager
MantÄ±ksal olarak her controller ayrÄ± bir sÃ¼reÃ§tir, ancak karmaÅŸÄ±klÄ±ÄŸÄ± azaltmak iÃ§in hepsi tek bir binary olarak derlenmiÅŸtir ve tek bir process olarak Ã§alÄ±ÅŸÄ±r. Bu controllerlarÄ±n bazÄ±larÄ± ÅŸunlardÄ±r:

- node controller
- job controller
- service account & token controller
- endpoints controller

K8S cluster'Ä±n mevcut durumuyla ondan istenen durum arasÄ±nda fark olup olmadÄ±ÄŸÄ±nÄ± gÃ¶zlemler. kube-apiserver aracÄ±lÄ±ÄŸÄ±yla etcd'de saklanan cluster durumunu izler. EÄŸer mevcut durum ve istenen durum arasÄ±nda  fark varsa bu farkÄ± oluÅŸturan kaynaklarÄ± gerektiÄŸi gibi oluÅŸturur, gÃ¼nceller veya silerek bu farkÄ± kapatÄ±r.

YukarÄ±da deÄŸindiÄŸimiz bu 4 komponent ( kube-apiserver, etcd, kube-scheduler, kube-control-manager) K8S'in yÃ¶netim ksÄ±mÄ±nda yer alÄ±r. Control plane olarak adlandÄ±rÄ±lan bu yÃ¶netim alanÄ± master node dediÄŸimiz sistemler Ã¼zerinde Ã§alÄ±ÅŸÄ±r.  TÃ¼m bu komponentler tek bir linux sistem Ã¼zerine de kurulabilir veya high availability (HA) bir yapÄ± saÄŸlamak isteniyorsa ayrÄ± sistemler Ã¼zerine de kurulabilir. Biraz sonra kendi localimizde nasÄ±l K8S cluster'Ä± daÄŸÄ±tabiliriz bundan bahsediyor olacaÄŸÄ±m.

## Worker Node BileÅŸenleri
Ä°ÅŸ yÃ¼klerimiz worker node dediÄŸimiz sistemler Ã¼zerinde koÅŸar. Ä°ÅŸ yÃ¼kÃ¼ dediÄŸimiz ÅŸey burada podlarÄ±mÄ±z yani containerlarÄ±mÄ±z oluyor. Ãœzerlerinde bir container rumtime (containerd, cri-o veya docker gibi) barÄ±ndÄ±rÄ±rlar.  Her worker node aÅŸaÄŸÄ±daki 3 temel komonenti barÄ±ndÄ±rÄ±r.

**- kubelet**
**- k-proxy**
**- container runtime** (VarsayÄ±lan dockerdÄ± bazÄ± nedenlerden dolayÄ± desteÄŸini bÄ±rakÄ±p containerd'ye geÃ§ti.)

Bu bileÅŸenleri inceleyelim.

### kubelet
kube-apiserver aracÄ±lÄ±ÄŸÄ±yla etcd'yi kontrol eder ve kube-scheduler tarafÄ±ndan bulunduÄŸu node Ã¼zerinde Ã§alÄ±ÅŸmasÄ± gereken pod belirtildiyse kubelet bu podu o node Ã¼zerinde oluÅŸturur. Bunu yaparken containerd' ye haber gÃ¶nderir ve belirtilen Ã¶zelliklerde bir container' Ä±n o sistemde Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.

### kube-proxy 
OluÅŸturulan podlarÄ±n aÄŸ kurallarÄ±nÄ± yÃ¶netir.  Bu aÄŸ kurallarÄ± cluster' Ä±n iÃ§indeki veya dÄ±ÅŸÄ±ndaki aÄŸ oturumlarÄ±ndan podlarÄ±nÄ±zla aÄŸ iletiÅŸimine izin verir. KÄ±sacasÄ± network proxy gÃ¶revi gÃ¶rÃ¼r.

### container runtime
ContainerlarÄ± Ã§alÄ±ÅŸtÄ±rmaktan sorumlu olan yazÄ±lÄ±mdÄ±r. Kubernetes; containerd, CRI-O gibi contaner runtimelarÄ± ve Kubernetes CRI'nÄ±n (Container Runtme  Interface) diÄŸer tÃ¼m uygulamalarÄ±nÄ± destekler.


Ä°ÅŸin sistem tarafÄ±nda yer alÄ±yor uzun vadede bir K8S ortamÄ± yÃ¶netiyorsanÄ±z bu yapÄ±yÄ± iyi anlamak ve Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± bilmek Ã¶nemlidir. 


# Kubernetes YÃ¶netim AraÃ§larÄ±
YukarÄ±da kube-apiserver isimli bir komponentten bahsetmiÅŸtik. TÃ¼m cluster kopmonentleri ve dÄ±ÅŸ dÃ¼nyadan bizler bu apiserver ile iletiÅŸime geÃ§iyorduk. K8S cluster Ã¼zerinde bir iÅŸlem yapmak istiyorsak yani k8s'e bir komut vermek istiyorsak bunu kube-apiserver Ã¼zerinde yapÄ±yoruz. Bunu gerÃ§ekleÅŸtirebilmenin ise 3 yÃ¶ntemi var:

**- Rest API Ã§aÄŸrÄ±larÄ±**  Ã¼zerinden iletiÅŸime geÃ§ebiliriz.  Fakat her iÅŸlemi bu ÅŸekilde gerÃ§ekleÅŸtirmek oldukÃ§a zor.
**-  GUI**  istemcileri (apiserver ile iletiÅŸim kurmak iÃ§in tasarlanmÄ±ÅŸ Ã§eÅŸitli arayÃ¼zler) ile iletiÅŸime geÃ§ebiliriz.
**-  Kubectl**    resmi CLI aracÄ± ile kubernetes cluster'Ä±na komutlar gÃ¶nderebiliriz.


Ã–rnek olarak bir Kubernetes Cluster'Ä± oluÅŸturmak istiyorum. Fakat Ã¶ncesinde kubectl aracÄ±nÄ± sistemimize kurmamÄ±z ve daha sonra  kubernetes cluster'Ä± kurmamÄ±z sonrasÄ±nda ikisi arasÄ±ndaki baÄŸlantÄ±yÄ± yapmak doÄŸru olacaktÄ±r.

Kubectl aracÄ±nÄ± kurmak iÃ§in [bu baÄŸlantÄ±dan](https://kubernetes.io/docs/tasks/tools/) sisteminize uygun yÃ¶nergeleri takip edebilirsiniz.

Ubuntu daÄŸÄ±tÄ±mÄ±nÄ± kullandÄ±ÄŸÄ±mdan Debian-based sistem Ã¼zerine native paket yÃ¶netimi metodunu kullanarak kuruluma yapacaÄŸÄ±m. Siz dilerseniz homebrew paket yÃ¶neticisi ile daha kolay kurulum yapabilirsiniz veya curl ile kubectl binary dosyasÄ±nÄ± yÃ¼kleyerek kubectl kurulumunu yapabilirisiniz. AdÄ±m adÄ±m gidelim.

* Var olan paketlerinizi gÃ¼ncelleyin ve k8s iÃ§in gerekli paketleri kurun.

``sudo apt update``

``sudo apt-get install -y ca-certificates curl ``

![]({{ 'assets/img/k8s1.PNG' | relative_url }})

* Debian 9  veya Ã¶nceki bir sÃ¼rÃ¼mÃ¼nÃ¼ kullanÄ±yorsanÄ±z apt-transport-https'yi de yÃ¼klemeniz gerekiyormuÅŸ. Benim Ã¶yle olduÄŸundan bunu da yÃ¼kledim :) 

``sudo apt-get install -y apt-transport-https``

![]({{ 'assets/img/k8s2.PNG' | relative_url }})

* Google Cloud public signing  key'ini de  yÃ¼kleyin. 

``sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg``

![]({{ 'assets/img/k8s3.PNG' | relative_url }})

* Daha sonra Kubernetes repository'sini ekleyelim.

```
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" /
| sudo tee /etc/apt/sources.list.d/kubernetes.list
```

![]({{ 'assets/img/k8s4.PNG' | relative_url }})

* Son adÄ±mda apt paket dizinini yeni depoyla gÃ¼ncelleyip ve kubectl'i kuralÄ±m.

``sudo apt-get update``

![]({{ 'assets/img/k8s5.PNG' | relative_url }})

``sudo apt-get install -y kubectl``

![]({{ 'assets/img/k8s6.PNG' | relative_url }})

* Åimdi kontol edelim kubectl aracÄ±nÄ± kurabilmiÅŸ  miyiz ve versiyonu ne?

``kubectl version --client --output=yaml``

![]({{ 'assets/img/k8s7.PNG' | relative_url }})

Kubectl aracÄ±nÄ± kurduk ÅŸimdi ise K8S cluster' Ä± nasÄ±l kurabileceÄŸimize bakalÄ±m. Burada karÅŸÄ±mÄ±za bir Ã§ok seÃ§enek Ã§Ä±kÄ±yor.  Test ve geliÅŸtirme gibi iÅŸlemler iÃ§in kendi localimize ufak bir k8s kurmak istiyorsak, minikube veya docker desktop en uygun araÃ§lar olabilir. EÄŸer bir kaÃ§ sanal makine oluÅŸturarak production sistemler gibi tam bir k8s cluster'Ä± oluÅŸturmak istiyorsanÄ±z kubeadm, kubespray, RKE ve k0s gibi bir Ã§ok aracÄ± deneyimleyebilirsiniz. AynÄ± zamanda public cloud firmalarÄ±nÄ±n sunduÄŸu (Azure Kubernetes Service, Amazon EKS, Google Kubernetes Engine) yÃ¶netilen k8s hizmetlerini de kullanabilirsiniz.

Ben kubeadm kullanarak kurulumu yapmak istiyorum, o sebeple bu yazÄ±mda bunu ele alacaÄŸÄ±m.

Ã–ncesinde kendi sanallaÅŸtÄ±rma ortamÄ±mda iki adet vm ayaÄŸa kaldÄ±racaÄŸÄ±m ve Ã¼zerine bir linux distrosu yÃ¼kleyeceÄŸim. Bu iki makineden birisi control plane olarak yapÄ±landÄ±rÄ±lacak diÄŸeri ise worker nodelarÄ± Ã¼zerinde barÄ±ndÄ±racak. Fakat burda tek tek vm oluÅŸturup iÅŸletim sistemi kurmayla vs uÄŸraÅŸmadan komut satÄ±rÄ±ndan kolayca sanal makine oluÅŸturmamÄ±zÄ± saÄŸlayan multipass aracÄ±nÄ± kullanacaÄŸÄ±m. [Bu ](https://multipass.run/install) adresten aracÄ± inceleyebilirisiniz.


![]({{ 'assets/img/multipass.PNG' | relative_url }})

Multipass aracÄ±nÄ± kullanarak parametreler ile belirlediÄŸimiz kaynaklara gÃ¶re iki tane sanal makine oluÅŸturalÄ±m.

``multipass launch --name masters -c 2 -m 2G -d 10G``

``multipass launch --name worker -c 2 -m 2G -d 10G``

Ä°ki makineyede baÄŸlanÄ±p shell alalÄ±m. 

![]({{ 'assets/img/multipassshell.PNG' | relative_url }})

Ä°ki makine iÃ§in de iptables bridge traffic ayarlarÄ±nÄ± da aÅŸaÄŸÄ±daki gibi yapalÄ±m.

```
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF 
```

```
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
```

![]({{ 'assets/img/iptables.PNG' | relative_url }})

`sudo sysctl --system`

Ä°kinci yapmamÄ±z gereken ise bu sistemlere bir container engine kurmak, ben containerd kuracaÄŸÄ±m. Bunun iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyelim.

```
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
```


```
sudo modprobe overlay
sudo modprobe br_netfilter
```


```
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

```

`sudo sysctl --system`

```
sudo apt-get update
sudo apt-get upgrade -y
sudo apt-get install containerd -y
sudo mkdir -p /etc/containerd
sudo su -
containerd config default | tee /etc/containerd/config.toml
exit
sudo systemctl restart containerd
```

Åimdi K8S cluster kurulumu iÃ§in bizim kullanacaÄŸÄ±mÄ±z kubeadm aracÄ±nÄ± kuralÄ±m.

```
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

Sonunda geldik kubernetes kurulumuna :) kur kur bitiremedik, ÅŸimdi hazÄ±rÄ±z.

AÅŸaÄŸÄ±daki iÅŸlemleri master node Ã¼zerinde yapmalÄ± ve worker node' u daha sonra cluster'a dahil etmelisiniz.

Ä°lk olarak k8s kurulumu iÃ§in gerekli olan imajlarÄ± Ã§ekelim.

`sudo kubeadm config images pull`

![]({{ 'assets/img/masterpull.PNG' | relative_url }})

AÅŸaÄŸÄ±daki init komutuyla bir k8s cluster oluÅŸturuyoruz. Bu komutu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±z yer master node olacak, kubeadm bu makineyi master node haline getirecek.  apiserver-advertise-address ve control-plane-endpoint master node makinesinin IP adresi, pod-network-cidr kÄ±smÄ±na bir sonraki yazÄ±mda ayrÄ±ntÄ±lÄ± deÄŸineceÄŸim, ÅŸimdilik kurulumu tamamlayalÄ±m.

`sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=<IP> --control-plane-endpoint=<IP>`

kubectl ile bu makineye baÄŸlanabilmek ve onu yÃ¶netebilmek gerekiyor. Bunun iÃ§in gerekli yapÄ±landÄ±rmalarÄ± yapalÄ±m.

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

![]({{ 'assets/img/kubectlmasters.PNG' | relative_url }})

GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi artÄ±k kubectl aracÄ± ile k8s cluster'Ä± ile iletiÅŸim kurabiliyorum.

![]({{ 'assets/img/mastersnotready.PNG' | relative_url }})

ArtÄ±k bu cluster'a master veya worker rolÃ¼ne sahip yeni node'lar ekleyebilirim.  AÅŸaÄŸÄ±da komutun Ã§Ä±ktÄ±sÄ±nda da bahsettiÄŸi gibi master node eklemek istersem yukarÄ±daki komutu kullanabilirim, eÄŸer worker node eklemek istersem aÅŸaÄŸÄ±da belirttiÄŸi komutu kullanabilirim. 

![]({{ 'assets/img/masterworker.PNG' | relative_url }})

Ben bir worker node dahil ediyorum. Bunun iÃ§in worker node Ã¼zerinde bana verdiÄŸi aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±yorum.

`kubeadm join 10.122.52.30:6443 --token 3ggru0.q59zb4knopl4rcm8 --discovery-token-ca-cert-hash/ sha256:a55bbc268e3a116dc9a32a2c5ffd98c5b5132ab7cad4d511d61c2e72a47a94e3`

BakalÄ±m dahil edebilmiÅŸ miyiz?

![]({{ 'assets/img/get nodes.PNG' | relative_url }})

Farkettiyseniz cluster ayakta ama node'lar henÃ¼z Ready duruma geÃ§memiÅŸ. Bunun nedeni kurduÄŸumuz cluster varsayÄ±lan olarak bir network altyapÄ±sÄ± ile gelmez. Bunun iÃ§in bir plugin kurmak gerekiyor. AÅŸaÄŸÄ±daki komutlarÄ± kullanarak bunu da kurabiliriz.

```
kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
```

Bir sÃ¼re sonra node statÃ¼lerinin Ready duruma geÃ§tiÄŸini gÃ¶receksiniz.  ArtÄ±k kurulumu tamamlamÄ±ÅŸ bulunuyoruz. 

![]({{ 'assets/img/ready.PNG' | relative_url }})

Bir sonraki yazÄ±mda pod, namespace, label ve selector, deployment, service, replicaset gibi bir Ã§ok kavrama deÄŸinmeyi dÃ¼ÅŸÃ¼nÃ¼yorum. Åimdilik gÃ¶rÃ¼ÅŸmek Ã¼zere diyorum :)


May The Kubernetes Be With You  ğŸŒŸ ğŸ˜‰

![]({{ 'assets/img/k8smeme.jpeg' | relative_url }})
